model:
  # Model architecture parameters
  name: "SmolLM2-135"
  hidden_dim: 768
  n_layers: 12
  n_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 2048
  vocab_size: 50258
  
  # Regularization parameters
  dropout: 0.1
  attention_dropout: 0.1
  
  # Numerical stability parameters
  layer_norm_epsilon: 1e-5
  initializer_range: 0.02
  
  # Model features
  use_cache: true
  use_flash_attention: true
  use_rotary_embeddings: true
  gradient_checkpointing: true

training:
  # Optimization parameters
  learning_rate: 5e-4
  weight_decay: 0.1
  max_grad_norm: 1.0
  
  # Training process parameters
  batch_size: 32
  gradient_accumulation_steps: 4
  
  # Training duration parameters
  warmup_steps: 100
  eval_steps: 500
  save_steps: 500
  prediction_steps: 500
  
  # Mixed precision settings
  mixed_precision: "bf16"
  num_workers: 4
  seed: 42

generation:
  max_length: 100
  temperature: 1.0
  top_k: 50
  top_p: 0.95
  do_sample: true
  num_samples: 3
  prompt_templates: [
    "Once upon a time",
    "In a world where",
    "The most interesting thing about"
  ]

wandb:
  # WandB logging configuration
  project: "smollm2-training"
  entity: null  # Set to your wandb username if needed
  log_model: true
  watch: "gradients"
  
  # Logging frequencies
  log_every: 500  # Log every 500 steps
  save_steps: 500
  
  # Text generation logging
  log_generations: true
  generation_frequency: 500  # Match with prediction_steps
  log_samples: true
  samples_to_log: 3
  
  # Metrics logging
  log_metrics: [
    "loss",
    "learning_rate",
    "perplexity",
    "generated_tokens"
  ]
  
  # Sample logging format
  sample_logging:
    log_table: true
    table_name: "generated_samples"
    columns: [
      "step",
      "prompt",
      "generated_text",
      "generation_time",
      "perplexity"
    ]
  
  # Visualization
  plots: [
    "loss_curve",
    "learning_rate_curve",
    "grad_flow",
    "sample_length_distribution"
  ]
  
  # Checkpoint handling
  save_checkpoints: true
  checkpoint_frequency: 500 