model:
  hidden_dim: 768
  n_layers: 12
  n_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 2048
  vocab_size: 32000
  dropout: 0.1
  attention_dropout: 0.1
  layer_norm_epsilon: 1.0e-5
  initializer_range: 0.02
  use_cache: true
  use_flash_attention: true
  use_rotary_embeddings: true
  gradient_checkpointing: true

training:
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  fp16: true
  bf16: true
  gradient_accumulation_steps: 4
  eval_steps: 500
  save_steps: 500
  prediction_steps: 500
  max_steps: 5000
  
tokenizer:
  type: "BPE"
  pad_token: "[PAD]"
  unk_token: "[UNK]"
  bos_token: "[BOS]"
  eos_token: "[EOS]"
  max_length: 1024

optimization:
  optimizer: "AdamW"
  scheduler: "cosine"
  lr_scheduler_type: "cosine"
  num_warmup_steps: 100 